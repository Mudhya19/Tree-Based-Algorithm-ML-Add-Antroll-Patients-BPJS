{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPJS Antrol Patients Analysis\n",
    "This notebook performs machine learning analysis on BPJS antrol patients data using tree-based algorithms to predict patient outcomes or patterns.\n",
    "\n",
    "## Proyek Klasifikasi (Supervised Learning)\n",
    "Tujuan: \"Analisis Komperenshif Identifikasi Pendaftaran Pasien BPJS Add Antroll\"\n",
    "Kasus: Analisis Klasifikasi Pasien BPJS Add Antrol\n",
    "\n",
    "### Persyaratan Proyek:\n",
    "- Definisi Masalah: Membantu manajemen rumah sakit memahami pola pendaftaran pasien BPJS dan memprediksi jenis pembayaran atau status kunjungan pasien berdasarkan data pendaftaran.\n",
    "- Kompleksitas Dataset: Dataset yang diproses memiliki campuran fitur numerik dan kategorikal serta menunjukkan proses preprocessing yang kompleks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understanding Business: Memahami kebutuhan bisnis dan tujuan analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permasalahan Utama:\n",
    "Analisis pola keberhasilan dan kegagalan pendaftaran pasien BPJS melalui dua kanal yaitu Anjungan Pendaftaran Mandiri (APM) dan aplikasi Mobile JKN. Melalui pendekatan analisis log dan metode clustering dalam penerapan machine learning dengan menggunakan Tree-Based Algorithm : Decision Tree, Random Forest dan Gradient Boosting. bertujuan untuk mengidentifikasi faktor-faktor penyebab dan memberikan rekomendasi peningkatan efektivitas pelayanan digital rumah sakit.\n",
    "\n",
    "Secara tidak optimalnya proses pendaftaran pasien BPJS pada kanal APM dan Mobile JKN karena adanya variasi tingkat keberhasilan dan kegagalan pengiriman data (status_kirim) yang menunjukkan pola berbeda, namun belum dianalisis secara komprehensif.\n",
    "\n",
    "## Kebutuhan Bisnis:\n",
    "1. Memahami faktor-faktor yang mempengaruhi keberhasilan pendaftaran BPJS melalui APM dan Mobile JKN\n",
    "2. Mengidentifikasi pola kegagalan pendaftaran untuk perbaikan sistem\n",
    "3. Memberikan rekomendasi untuk meningkatkan efektivitas layanan digital rumah sakit\n",
    "4. Membantu manajemen dalam pengambilan keputusan terkait layanan pendaftaran pasien BPJS\n",
    "\n",
    "## Tujuan Analisis:\n",
    "- Mengklasifikasikan keberhasilan/kegagalan pendaftaran pasien BPJS berdasarkan berbagai faktor\n",
    "- Menganalisis perbedaan pola pendaftaran antara kanal APM dan Mobile JKN\n",
    "- Mengidentifikasi variabel-variabel penting yang mempengaruhi keberhasilan pendaftaran\n",
    "- Membangun model prediktif untuk membantu pengambilan keputusan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'database'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder, StandardScaler\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatabase\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatabase_connection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_bpjs_antrol_data\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'database'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from database.database_connection import get_bpjs_antrol_data\n",
    "from config.config import Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "import logging\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set up logging\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/bpjs_antrol_analysis.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"1. Understanding Business: Analyzing BPJS patient registration patterns to predict payment type or visit status\")\n",
    "logger.info(\"Business Goal: Help hospital management understand BPJS registration patterns and optimize resource allocation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Understanding: Menjelajahi dan memahami struktur data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_csv(file_path):\n",
    "    # Load patient data from CSV file\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        logger.info(f\"Data loaded successfully from {file_path}, shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data from {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_data_from_db_fallback():\n",
    "# Load patient data from database with CSV fallback\n",
    "    config = Config()\n",
    "    \n",
    "    # Try to load from database first\n",
    "    try:\n",
    "        logger.info(\"Attempting to load data from database...\")\n",
    "        from datetime import datetime, timedelta\n",
    "        # Set date range for 2025, from January to June\n",
    "        start_date = '2025-01-01'\n",
    "        end_date = '2025-06-30'\n",
    "        \n",
    "        df = get_bpjs_antrol_data(start_date, end_date)\n",
    "        logger.info(f\"Successfully loaded {len(df)} records from database\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Database connection failed: {e}\")\n",
    "        logger.info(\"Loading data from CSV fallback...\")\n",
    "        \n",
    "        # Fallback to CSV file\n",
    "        csv_file_path = os.path.join('database', \"bpjs antrol.csv\")\n",
    "        if os.path.exists(csv_file_path):\n",
    "            df = load_data_from_csv(csv_file_path)\n",
    "            logger.info(\"Loaded data from CSV fallback successfully\")\n",
    "            # Filter data for the required date range (2025-01-01 to 2025-06-30)\n",
    "            if 'tgl_registrasi' in df.columns:\n",
    "                df['tgl_registrasi'] = pd.to_datetime(df['tgl_registrasi'], format='mixed', dayfirst=True, errors='coerce')\n",
    "                df = df[(df['tgl_registrasi'] >= '2025-01-01') & (df['tgl_registrasi'] <= '2025-06-30')]\n",
    "                logger.info(f\"Filtered CSV data for date range 2025-01-01 to 2025-06-30, shape: {df.shape}\")\n",
    "            return df\n",
    "        else:\n",
    "            logger.error(f\"CSV file not found at {csv_file_path}\")\n",
    "            raise FileNotFoundError(f\"Neither database connection nor CSV file available\")\n",
    "\n",
    "logger.info(\"2. Data Understanding: Exploring and understanding data structure...\")\n",
    "try:\n",
    "    df = load_data_from_db_fallback()\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Data loading failed: {e}\")\n",
    "\n",
    "logger.info(f\"Dataset loaded with shape: {df.shape}\")\n",
    "logger.info(f\"Dataset columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation / Wrangling: Menyiapkan dan mengolah data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"Create additional features from the raw data\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert date columns to datetime if they exist\n",
    "    date_columns = ['tgl_registrasi', 'tanggal_periksa']\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Create additional features\n",
    "    if 'tgl_registrasi' in df.columns:\n",
    "        df['hari_registrasi'] = df['tgl_registrasi'].dt.day_name()\n",
    "        df['bulan_registrasi'] = df['tgl_registrasi'].dt.month\n",
    "        df['tahun_registrasi'] = df['tgl_registrasi'].dt.year\n",
    "    \n",
    "    if 'tanggal_periksa' in df.columns:\n",
    "        df['hari_periksa'] = df['tanggal_periksa'].dt.day_name()\n",
    "        df['bulan_periksa'] = df['tanggal_periksa'].dt.month\n",
    "        df['tahun_periksa'] = df['tanggal_periksa'].dt.year\n",
    "\n",
    "    # Calculate difference between registration and examination dates\n",
    "    if 'tgl_registrasi' in df.columns and 'tanggal_periksa' in df.columns:\n",
    "        df['hari_antara_reg_periksa'] = (df['tanggal_periksa'] - df['tgl_registrasi']).dt.days\n",
    "\n",
    "    return df\n",
    "\n",
    "logger.info(\"3. Data Preparation/Wrangling: Preparing and wrangling the dataset...\")\n",
    "logger.info(f\"Original dataset shape: {df.shape}\")\n",
    "logger.info(f\"Original dataset columns: {list(df.columns)}\")\n",
    "\n",
    "# Create additional features for better analysis\n",
    "original_columns = set(df.columns)\n",
    "df = create_features(df)\n",
    "logger.info(f\"Dataset shape after feature creation: {df.shape}\")\n",
    "new_columns = set(df.columns) - original_columns\n",
    "if new_columns:\n",
    "    logger.info(f\"New columns added: {new_columns}\")\n",
    "else:\n",
    "    logger.info(\"No new columns were added during feature creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Cleaning: Membersihkan data dari ketidakkonsistenan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"4. Data Cleaning: Cleaning data from inconsistencies...\")\n",
    "\n",
    "# Check for missing values before cleaning\n",
    "missing_before = df.isnull().sum()\n",
    "logger.info(f\"Missing values before cleaning:\\n{missing_before[missing_before > 0]}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "initial_count = len(df)\n",
    "df = df.drop_duplicates()\n",
    "duplicates_removed = initial_count - len(df)\n",
    "if duplicates_removed > 0:\n",
    "    logger.info(f\"Removed {duplicates_removed} duplicate rows\")\n",
    "else:\n",
    "    logger.info(\"No duplicate rows found\")\n",
    "\n",
    "# Handle missing values with more sophisticated approach\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        if df[col].dtype in ['object', 'category']:\n",
    "            # For categorical columns, fill with mode or 'Unknown'\n",
    "            mode_value = df[col].mode()\n",
    "            if not mode_value.empty:\n",
    "                df[col].fillna(mode_value[0], inplace=True)\n",
    "            else:\n",
    "                df[col].fillna('Unknown', inplace=True)\n",
    "        else:\n",
    "            # For numerical columns, fill with median\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# Identify and handle potential outliers in numerical columns using IQR method\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for col in numerical_cols:\n",
    "    if col not in ['tahun_registrasi', 'tahun_periksa']:  # Exclude year columns from outlier detection\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers_count = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "        if outliers_count > 0:\n",
    "            logger.info(f\"Found {outliers_count} outliers in column '{col}'\")\n",
    "            # Note: We'll keep outliers for now but flag them for analysis\n",
    "\n",
    "# Check for missing values after cleaning\n",
    "missing_after = df.isnull().sum()\n",
    "logger.info(f\"Missing values after cleaning:\\n{missing_after[missing_after > 0]}\")\n",
    "\n",
    "logger.info(f\"Dataset shape after cleaning: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Explanatory Data Analysis (EDA Deskriptif): Analisis deskriptif awal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_initial_eda(df):\n",
    "    \"\"\"Perform initial Exploratory Data Analysis\"\"\"\n",
    "    logger.info(\"Initial EDA - Basic dataset information:\")\n",
    "    logger.info(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    logger.info(\"Numerical columns summary:\")\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if numerical_cols:\n",
    "        logger.info(f\"\\n{df[numerical_cols].describe()}\")\n",
    "    \n",
    "    # Categorical columns summary\n",
    "    logger.info(\"Categorical columns summary:\")\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    for col in categorical_cols[:5]: # Limit to first 5 categorical columns\n",
    "        logger.info(f\"\\nValue counts for {col}:\")\n",
    "        logger.info(f\"{df[col].value_counts().head()}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_data = missing_data[missing_data > 0]\n",
    "    if not missing_data.empty:\n",
    "        logger.info(f\"Columns with missing values:\\n{missing_data}\")\n",
    "    else:\n",
    "        logger.info(\"No missing values found in the dataset\")\n",
    "\n",
    "def perform_eda_visualizations(df):\n",
    "    \"\"\"Perform EDA visualizations for the dataset - each visualization saved separately\"\"\"\n",
    "    logger.info(\"Starting EDA visualizations...\")\n",
    "    \n",
    "    # Check if dataframe is empty\n",
    "    if df.empty:\n",
    "        logger.warning(\"Dataset is empty, skipping EDA visualizations\")\n",
    "        return\n",
    "    \n",
    "    # Create image directory if it doesn't exist\n",
    "    os.makedirs('image', exist_ok=True)\n",
    "    \n",
    "    # 1. Distribution of registration status (status_kirim)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if 'status_kirim' in df.columns:\n",
    "        status_counts = df['status_kirim'].value_counts()\n",
    "        if not status_counts.empty:\n",
    "            plt.pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "            plt.title('Distribution of Registration Status (status_kirim)')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for status_kirim', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Distribution of Registration Status (status_kirim)')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'status_kirim column not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Distribution of Registration Status (status_kirim)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/eda_status_kirim.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()  # Close the figure to free memory\n",
    "    logger.info(\"Saved EDA visualization: eda_status_kirim.png\")\n",
    "    \n",
    "    # 2. Distribution of visit types (jenis_kunjungan)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if 'jenis_kunjungan' in df.columns:\n",
    "        visit_counts = df['jenis_kunjungan'].value_counts()\n",
    "        if not visit_counts.empty:\n",
    "            plt.bar(range(len(visit_counts)), visit_counts.values)\n",
    "            plt.title('Distribution of Visit Types (jenis_kunjungan)')\n",
    "            plt.xlabel('Visit Type')\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks(range(len(visit_counts)), visit_counts.index.astype(str), rotation=45)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for jenis_kunjungan', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Distribution of Visit Types (jenis_kunjungan)')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'jenis_kunjungan column not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Distribution of Visit Types (jenis_kunjungan)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/eda_jenis_kunjungan.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close() # Close the figure to free memory\n",
    "    logger.info(\"Saved EDA visualization: eda_jenis_kunjungan.png\")\n",
    "    \n",
    "    # 3. Distribution of registration status (status_lanjut)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if 'status_lanjut' in df.columns:\n",
    "        status_lanjut_counts = df['status_lanjut'].value_counts()\n",
    "        if not status_lanjut_counts.empty:\n",
    "            plt.bar(range(len(status_lanjut_counts)), status_lanjut_counts.values)\n",
    "            plt.title('Distribution of Registration Status (status_lanjut)')\n",
    "            plt.xlabel('Status Lanjut')\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks(range(len(status_lanjut_counts)), status_lanjut_counts.index, rotation=45)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for status_lanjut', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Distribution of Registration Status (status_lanjut)')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'status_lanjut column not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Distribution of Registration Status (status_lanjut)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/eda_status_lanjut.png', dpi=30, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close() # Close the figure to free memory\n",
    "    logger.info(\"Saved EDA visualization: eda_status_lanjut.png\")\n",
    "    \n",
    "    # 4. Distribution of registration by day of week\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if 'hari_registrasi' in df.columns:\n",
    "        day_counts = df['hari_registrasi'].value_counts()\n",
    "        if not day_counts.empty:\n",
    "            # Order days of week properly\n",
    "            days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "            ordered_counts = day_counts.reindex(days_order, fill_value=0)\n",
    "            plt.bar(range(len(ordered_counts)), ordered_counts.values)\n",
    "            plt.title('Distribution of Registrations by Day of Week')\n",
    "            plt.xlabel('Day of Week')\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks(range(len(ordered_counts)), ordered_counts.index, rotation=45)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for hari_registrasi', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Distribution of Registrations by Day of Week')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'hari_registrasi column not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Distribution of Registrations by Day of Week')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/eda_hari_registrasi.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close() # Close the figure to free memory\n",
    "    logger.info(\"Saved EDA visualization: eda_hari_registrasi.png\")\n",
    "    \n",
    "    # 5. Distribution of registration by month\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if 'bulan_registrasi' in df.columns:\n",
    "        month_counts = df['bulan_registrasi'].value_counts()\n",
    "        if not month_counts.empty:\n",
    "            # Order months properly\n",
    "            months_order = list(range(1, 13))\n",
    "            ordered_counts = month_counts.reindex(months_order, fill_value=0)\n",
    "            plt.bar(range(len(ordered_counts)), ordered_counts.values)\n",
    "            plt.title('Distribution of Registrations by Month')\n",
    "            plt.xlabel('Month')\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks(range(len(ordered_counts)), ordered_counts.index, rotation=45)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for bulan_registrasi', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Distribution of Registrations by Month')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'bulan_registrasi column not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Distribution of Registrations by Month')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/eda_bulan_registrasi.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()  # Close the figure to free memory\n",
    "    logger.info(\"Saved EDA visualization: eda_bulan_registrasi.png\")\n",
    "    \n",
    "    # 6. Distribution of registration by poli\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if 'nm_poli' in df.columns:\n",
    "        # Get top 10 poli by count\n",
    "        poli_counts = df['nm_poli'].value_counts().head(10)\n",
    "        if not poli_counts.empty:\n",
    "            plt.barh(range(len(poli_counts)), poli_counts.values)\n",
    "            plt.title('Top 10 Poli by Registration Count')\n",
    "            plt.xlabel('Count')\n",
    "            plt.ylabel('Poli Name')\n",
    "            plt.yticks(range(len(poli_counts)), poli_counts.index)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for nm_poli', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Top 10 Poli by Registration Count')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'nm_poli column not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Top 10 Poli by Registration Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/eda_nm_poli.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close() # Close the figure to free memory\n",
    "    logger.info(\"Saved EDA visualization: eda_nm_poli.png\")\n",
    "    \n",
    "    # 7. Distribution of payment methods (png_jawab)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if 'png_jawab' in df.columns:\n",
    "        # Get top payment methods\n",
    "        payment_counts = df['png_jawab'].value_counts().head(10)\n",
    "        if not payment_counts.empty:\n",
    "            plt.barh(range(len(payment_counts)), payment_counts.values)\n",
    "            plt.title('Top 10 Payment Methods (png_jawab)')\n",
    "            plt.xlabel('Count')\n",
    "            plt.ylabel('Payment Method')\n",
    "            plt.yticks(range(len(payment_counts)), payment_counts.index)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for png_jawab', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Top 10 Payment Methods (png_jawab)')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'png_jawab column not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Top 10 Payment Methods (png_jawab)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/eda_png_jawab.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close() # Close the figure to free memory\n",
    "    logger.info(\"Saved EDA visualization: eda_png_jawab.png\")\n",
    "    \n",
    "    # 8. Registration status vs Visit type (if both columns exist)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if 'status_kirim' in df.columns and 'jenis_kunjungan' in df.columns:\n",
    "        crosstab = pd.crosstab(df['jenis_kunjungan'], df['status_kirim'])\n",
    "        if not crosstab.empty and crosstab.shape[0] > 0 and crosstab.shape[1] > 0:\n",
    "            sns.heatmap(crosstab, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title('Registration Status vs Visit Type')\n",
    "            plt.xlabel('Registration Status')\n",
    "            plt.ylabel('Visit Type')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for cross-tabulation', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Registration Status vs Visit Type')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'status_kirim and/or jenis_kunjungan columns not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Registration Status vs Visit Type')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/eda_status_vs_jenis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close() # Close the figure to free memory\n",
    "    logger.info(\"Saved EDA visualization: eda_status_vs_jenis.png\")\n",
    "    \n",
    "    # 9. Registration status vs Poli (top 10 poli)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    if 'status_kirim' in df.columns and 'nm_poli' in df.columns:\n",
    "        top_10_poli = df['nm_poli'].value_counts().head(10).index\n",
    "        df_top_poli = df[df['nm_poli'].isin(top_10_poli)]\n",
    "        if not df_top_poli.empty:\n",
    "            crosstab = pd.crosstab(df_top_poli['nm_poli'], df_top_poli['status_kirim'])\n",
    "            if not crosstab.empty and crosstab.shape[0] > 0 and crosstab.shape[1] > 0:\n",
    "                sns.heatmap(crosstab, annot=True, fmt='d', cmap='Blues')\n",
    "                plt.title('Registration Status vs Top 10 Poli')\n",
    "                plt.xlabel('Registration Status')\n",
    "                plt.ylabel('Poli Name')\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'No data for cross-tabulation', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "                plt.title('Registration Status vs Top 10 Poli')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for top 10 poli', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Registration Status vs Top 10 Poli')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'status_kirim and/or nm_poli columns not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Registration Status vs Top 10 Poli')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/eda_status_vs_poli.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close() # Close the figure to free memory\n",
    "    logger.info(\"Saved EDA visualization: eda_status_vs_poli.png\")\n",
    "\n",
    "# Perform initial EDA after feature creation\n",
    "perform_initial_eda(df)\n",
    "\n",
    "# Perform EDA visualizations\n",
    "logger.info(\"5. Explanatory Data Analysis (EDA Deskriptif): Performing basic EDA visualizations...\")\n",
    "perform_eda_visualizations(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Exploratory Data Analysis (EDA Mendalam): Eksplorasi data secara mendalam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_advanced_eda_visualizations(df):\n",
    "    \"\"\"Perform advanced EDA visualizations for the dataset - each visualization saved separately\"\"\"\n",
    "    logger.info(\"Starting advanced EDA visualizations...\")\n",
    "    \n",
    "    # Check if dataframe is empty\n",
    "    if df.empty:\n",
    "        logger.warning(\"Dataset is empty, skipping advanced EDA visualizations\")\n",
    "        return\n",
    "    \n",
    "    # Create image directory if it doesn't exist\n",
    "    os.makedirs('image', exist_ok=True)\n",
    "    \n",
    "    # 1. Registration timeline - registrations over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    if 'tgl_registrasi' in df.columns:\n",
    "        df['tgl_registrasi'] = pd.to_datetime(df['tgl_registrasi'], errors='coerce')\n",
    "        daily_counts = df.groupby(df['tgl_registrasi'].dt.date).size()\n",
    "        if not daily_counts.empty:\n",
    "            plt.plot(daily_counts.index, daily_counts.values)\n",
    "            plt.title('Daily Registration Trend')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Number of Registrations')\n",
    "            plt.xticks(rotation=45)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for timeline', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Daily Registration Trend')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'tgl_registrasi column not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Daily Registration Trend')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/advanced_eda_timeline.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()  # Close the figure to free memory\n",
    "    logger.info(\"Saved advanced EDA visualization: advanced_eda_timeline.png\")\n",
    "    \n",
    "    # 2. Box plot of time difference between registration and examination\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if 'hari_antara_reg_periksa' in df.columns:\n",
    "        data_to_plot = df['hari_antara_reg_periksa'].dropna()\n",
    "        if not data_to_plot.empty:\n",
    "            plt.boxplot(data_to_plot)\n",
    "            plt.title('Distribution of Days Between Registration and Examination')\n",
    "            plt.ylabel('Days')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for time difference', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Distribution of Days Between Registration and Examination')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'hari_antara_reg_periksa column not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Distribution of Days Between Registration and Examination')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/advanced_eda_time_diff.png', dpi=30, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()  # Close the figure to free memory\n",
    "    logger.info(\"Saved advanced EDA visualization: advanced_eda_time_diff.png\")\n",
    "    \n",
    "    # 3. Success rate by day of week\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if 'hari_registrasi' in df.columns and 'status_kirim' in df.columns:\n",
    "        success_rate_by_day = df.groupby('hari_registrasi')['status_kirim'].apply(\n",
    "            lambda x: (x == 'Sudah').sum() / len(x) * 10 if len(x) > 0 else 0\n",
    "        ).reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "        if not success_rate_by_day.dropna().empty:\n",
    "            plt.bar(range(len(success_rate_by_day)), success_rate_by_day.values)\n",
    "            plt.title('Registration Success Rate by Day of Week')\n",
    "            plt.xlabel('Day of Week')\n",
    "            plt.ylabel('Success Rate (%)')\n",
    "            plt.xticks(range(len(success_rate_by_day)), success_rate_by_day.index, rotation=45)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for success rate by day', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Registration Success Rate by Day of Week')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'hari_registrasi and/or status_kirim columns not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Registration Success Rate by Day of Week')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/advanced_eda_success_by_day.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()  # Close the figure to free memory\n",
    "    logger.info(\"Saved advanced EDA visualization: advanced_eda_success_by_day.png\")\n",
    "    \n",
    "    # 4. Success rate by month\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if 'bulan_registrasi' in df.columns and 'status_kirim' in df.columns:\n",
    "        success_rate_by_month = df.groupby('bulan_registrasi')['status_kirim'].apply(\n",
    "            lambda x: (x == 'Sudah').sum() / len(x) * 100 if len(x) > 0 else 0\n",
    "        )\n",
    "        if not success_rate_by_month.empty:\n",
    "            plt.bar(range(len(success_rate_by_month)), success_rate_by_month.values)\n",
    "            plt.title('Registration Success Rate by Month')\n",
    "            plt.xlabel('Month')\n",
    "            plt.ylabel('Success Rate (%)')\n",
    "            plt.xticks(range(len(success_rate_by_month)), success_rate_by_month.index, rotation=45)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for success rate by month', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Registration Success Rate by Month')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'bulan_registrasi and/or status_kirim columns not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Registration Success Rate by Month')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/advanced_eda_success_by_month.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()  # Close the figure to free memory\n",
    "    logger.info(\"Saved advanced EDA visualization: advanced_eda_success_by_month.png\")\n",
    "    \n",
    "    # 5. Success rate by poli (top 10)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if 'nm_poli' in df.columns and 'status_kirim' in df.columns:\n",
    "        top_10_poli = df['nm_poli'].value_counts().head(10).index\n",
    "        df_top_poli = df[df['nm_poli'].isin(top_10_poli)]\n",
    "        if not df_top_poli.empty:\n",
    "            success_rate_by_poli = df_top_poli.groupby('nm_poli')['status_kirim'].apply(\n",
    "                lambda x: (x == 'Sudah').sum() / len(x) * 100 if len(x) > 0 else 0\n",
    "            ).sort_values(ascending=True)\n",
    "            if not success_rate_by_poli.empty:\n",
    "                plt.barh(range(len(success_rate_by_poli)), success_rate_by_poli.values)\n",
    "                plt.title('Registration Success Rate by Top 10 Poli')\n",
    "                plt.xlabel('Success Rate (%)')\n",
    "                plt.ylabel('Poli Name')\n",
    "                plt.yticks(range(len(success_rate_by_poli)), success_rate_by_poli.index)\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'No data for success rate by poli', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "                plt.title('Registration Success Rate by Top 10 Poli')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for top 10 poli', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Registration Success Rate by Top 10 Poli')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'nm_poli and/or status_kirim columns not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Registration Success Rate by Top 10 Poli')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/advanced_eda_success_by_poli.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close() # Close the figure to free memory\n",
    "    logger.info(\"Saved advanced EDA visualization: advanced_eda_success_by_poli.png\")\n",
    "    \n",
    "    # 6. Distribution of numerical features if available\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numerical_cols) > 0:\n",
    "        # Remove any columns that are actually categorical but encoded as numbers\n",
    "        numerical_cols = [col for col in numerical_cols if col not in ['jenis_kunjungan', 'kd_dokter', 'no_rkm_medis']]\n",
    "        if len(numerical_cols) > 0:\n",
    "            col_to_plot = numerical_cols[0] # Plot the first numerical column\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            data_to_plot = df[col_to_plot].dropna()\n",
    "            if not data_to_plot.empty:\n",
    "                plt.hist(data_to_plot, bins=30, edgecolor='black')\n",
    "                plt.title(f'Distribution of {col_to_plot}')\n",
    "                plt.xlabel(col_to_plot)\n",
    "                plt.ylabel('Frequency')\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'No data for histogram', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "                plt.title('Distribution of Numerical Features')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'image/advanced_eda_distribution_{col_to_plot}.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            plt.close()  # Close the figure to free memory\n",
    "            logger.info(f\"Saved advanced EDA visualization: advanced_eda_distribution_{col_to_plot}.png\")\n",
    "        else:\n",
    "            logger.info(\"No suitable numerical columns for histogram\")\n",
    "    else:\n",
    "        logger.info(\"No numerical columns available\")\n",
    "    \n",
    "    # 7. Correlation heatmap for numerical features\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numerical_cols) > 1:\n",
    "        # Only include columns that are actually numerical, not categorical encoded as numbers\n",
    "        actual_numerical = [col for col in numerical_cols if col not in ['jenis_kunjungan', 'kd_dokter', 'no_rkm_medis']]\n",
    "        if len(actual_numerical) > 1:\n",
    "            corr_matrix = df[actual_numerical].corr()\n",
    "            if not corr_matrix.isna().all().all():  # Check if correlation matrix has valid values\n",
    "                sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "                plt.title('Correlation Heatmap of Numerical Features')\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'No valid correlation data', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "                plt.title('Correlation Heatmap of Numerical Features')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'Not enough numerical features for correlation', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Correlation Heatmap of Numerical Features')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Not enough numerical features for correlation', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Correlation Heatmap of Numerical Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/advanced_eda_correlation.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()  # Close the figure to free memory\n",
    "    logger.info(\"Saved advanced EDA visualization: advanced_eda_correlation.png\")\n",
    "    \n",
    "    # 8. Success vs Failure by time of day (using jam_reg if available)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    if 'jam_reg' in df.columns and 'status_kirim' in df.columns:\n",
    "        # Convert jam_reg to hour if it's in time format\n",
    "        df['jam_reg_hour'] = pd.to_datetime(df['jam_reg'], format='%H:%M:%S', errors='coerce').dt.hour\n",
    "        df_success = df[df['status_kirim'] == 'Sudah']\n",
    "        df_failure = df[df['status_kirim'] == 'Gagal']\n",
    "        success_by_hour = df_success['jam_reg_hour'].value_counts().sort_index()\n",
    "        failure_by_hour = df_failure['jam_reg_hour'].value_counts().sort_index()\n",
    "        if not success_by_hour.empty or not failure_by_hour.empty:\n",
    "            plt.plot(success_by_hour.index, success_by_hour.values, label='Success', marker='o')\n",
    "            plt.plot(failure_by_hour.index, failure_by_hour.values, label='Failure', marker='o')\n",
    "            plt.title('Success vs Failure by Hour of Registration')\n",
    "            plt.xlabel('Hour of Day')\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend()\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No data for success/failure by hour', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Success vs Failure by Hour of Registration')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'jam_reg and/or status_kirim columns not available', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Success vs Failure by Hour of Registration')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('image/advanced_eda_success_failure_by_hour.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close() # Close the figure to free memory\n",
    "    logger.info(\"Saved advanced EDA visualization: advanced_eda_success_failure_by_hour.png\")\n",
    "    \n",
    "    # 9. Pairplot of key numerical features (if available)\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numerical_cols) > 1:\n",
    "        actual_numerical = [col for col in numerical_cols if col not in ['jenis_kunjungan', 'kd_dokter', 'no_rkm_medis']]\n",
    "        if len(actual_numerical) >= 2:\n",
    "            # Sample the data if it's too large for pairplot\n",
    "            df_sample = df[actual_numerical].dropna().sample(min(100, len(df)), random_state=42)\n",
    "            if not df_sample.empty and len(df_sample) > 1:\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.scatterplot(data=df_sample, x=actual_numerical[0], y=actual_numerical[1])\n",
    "                plt.title(f'Scatter Plot: {actual_numerical[0]} vs {actual_numerical[1]}')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'image/advanced_eda_scatter_{actual_numerical[0]}_vs_{actual_numerical[1]}.png', dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                plt.close()  # Close the figure to free memory\n",
    "                logger.info(f\"Saved advanced EDA visualization: advanced_eda_scatter_{actual_numerical[0]}_vs_{actual_numerical[1]}.png\")\n",
    "            else:\n",
    "                logger.info(\"Not enough data for scatter plot\")\n",
    "        else:\n",
    "            logger.info(\"Not enough numerical features for scatter plot\")\n",
    "    else:\n",
    "        logger.info(\"Not enough numerical features for scatter plot\")\n",
    "\n",
    "logger.info(\"6. Exploratory Data Analysis (EDA Mendalam): Performing advanced EDA visualizations...\")\n",
    "perform_advanced_eda_visualizations(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Data Preprocessing: Pra-pemrosesan data untuk modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess the data for machine learning\"\"\"\n",
    "    logger.info(\"Starting data preprocessing...\")\n",
    "    \n",
    "    # Create features\n",
    "    df = create_features(df)\n",
    "    \n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    logger.info(f\"Categorical columns: {categorical_columns}\")\n",
    "    logger.info(f\"Numerical columns: {numerical_columns}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in numerical_columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown', inplace=True)\n",
    "    \n",
    "    # Convert timedelta columns to seconds for scaling\n",
    "    timedelta_columns = df.select_dtypes(include=['timedelta64[ns]']).columns\n",
    "    for col in timedelta_columns:\n",
    "        df[col] = df[col].dt.total_seconds()\n",
    "    \n",
    "    # Prepare features for modeling\n",
    "    # Select relevant features for modeling (excluding the target to prevent data leakage)\n",
    "    feature_columns = []\n",
    "    \n",
    "    # Add numerical features\n",
    "    feature_columns.extend([col for col in numerical_columns if col not in ['tahun_registrasi', 'tahun_periksa']])\n",
    "    \n",
    "    # Add categorical features that are relevant but not the target\n",
    "    relevant_categorical = ['nm_poli', 'hari_registrasi', 'status_lanjut', 'jenis_kunjungan', 'kd_dokter', 'kd_poli']\n",
    "    for col in relevant_categorical:\n",
    "        if col in df.columns and col != 'png_jawab':  # Exclude target variable from features\n",
    "            feature_columns.append(col)\n",
    "    \n",
    "    logger.info(f\"Selected {len(feature_columns)} features for modeling: {feature_columns}\")\n",
    "    \n",
    "    # Prepare X (features) and y (target)\n",
    "    # Remove target variable from features if it's in the feature list to prevent data leakage\n",
    "    if 'png_jawab' in feature_columns:\n",
    "        feature_columns.remove('png_jawab')\n",
    "        logger.info(\"Removed 'png_jawab' from features to prevent data leakage\")\n",
    "    \n",
    "    X = df[feature_columns].copy()\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in X.columns:\n",
    "        if pd.api.types.is_object_dtype(X[col]):\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Create target variable - based on the NOTES NOTEBOOK.md, we should focus on\n",
    "    # analyzing success/failure of registration (status_kirim) to identify patterns\n",
    "    # in APM and Mobile JKN channels\n",
    "    if 'status_kirim' in df.columns:\n",
    "        target_encoder = LabelEncoder()\n",
    "        y = target_encoder.fit_transform(df['status_kirim'].astype(str))\n",
    "        logger.info(f\"Created target variable from 'status_kirim' with {len(target_encoder.classes_)} classes: {target_encoder.classes_}\")\n",
    "        \n",
    "        # Remove the target column from feature_columns if it's in the list\n",
    "        if 'status_kirim' in feature_columns:\n",
    "            feature_columns.remove('status_kirim')\n",
    "            logger.info(\"Removed 'status_kirim' from features to prevent data leakage\")\n",
    "            \n",
    "        # Rebuild X with the corrected feature list\n",
    "        X = df[feature_columns].copy()\n",
    "        \n",
    "        # Re-encode categorical variables after rebuilding X\n",
    "        label_encoders = {}\n",
    "        for col in X.columns:\n",
    "            if pd.api.types.is_object_dtype(X[col]):\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "                label_encoders[col] = le\n",
    "    elif 'png_jawab' in df.columns:\n",
    "        # Fallback to payment method if status_kirim is not available\n",
    "        target_encoder = LabelEncoder()\n",
    "        y = target_encoder.fit_transform(df['png_jawab'].astype(str))\n",
    "        logger.info(f\"Created target variable from 'png_jawab' with {len(target_encoder.classes_)} classes: {target_encoder.classes_}\")\n",
    "        \n",
    "        # Remove the target column from feature_columns if it's in the list\n",
    "        if 'png_jawab' in feature_columns:\n",
    "            feature_columns.remove('png_jawab')\n",
    "            logger.info(\"Removed 'png_jawab' from features to prevent data leakage\")\n",
    "            \n",
    "        # Rebuild X with the corrected feature list\n",
    "        X = df[feature_columns].copy()\n",
    "        \n",
    "        # Re-encode categorical variables after rebuilding X\n",
    "        label_encoders = {}\n",
    "        for col in X.columns:\n",
    "            if pd.api.types.is_object_dtype(X[col]):\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "                label_encoders[col] = le\n",
    "    else:\n",
    "        # Create a default binary target if neither status_kirim nor png_jawab is available\n",
    "        y = np.random.randint(0, 2, size=len(df))\n",
    "        target_encoder = None\n",
    "        logger.info(\"Created random binary target variable\")\n",
    "    \n",
    "    logger.info(f\"Final feature matrix shape: {X.shape}\")\n",
    "    # Ensure y is a numpy array to have shape attribute\n",
    "    y = np.array(y)\n",
    "    logger.info(f\"Target vector shape: {y.shape}\")\n",
    "    \n",
    "    return X, y, label_encoders, target_encoder\n",
    "\n",
    "logger.info(\"7. Data Preprocessing: Starting data preprocessing for modeling...\")\n",
    "X, y, label_encoders, target_encoder = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training Modeling: melatih model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree_models(X_train, y_train):\n",
    "    \"\"\"Train tree-based models as specified in the requirements:\n",
    "    Klasifikasi: Tree-Based Algorithm menggunakan model Machine Learning\n",
    "    (Decision Tree, Random Forest dan Gradient Boosting)\"\"\"\n",
    "    models = {\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100, max_depth=5)\n",
    "    }\n",
    "    \n",
    "    trained_models = {}\n",
    "    for name, model in models.items():\n",
    "        logger.info(f\"Training {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        trained_models[name] = model\n",
    "        logger.info(f\"{name} training completed\")\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "# Split the data with stratification if possible\n",
    "# Check if stratification is possible (each class has at least 2 samples)\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "min_samples_per_class = min(counts)\n",
    "\n",
    "if min_samples_per_class >= 2:\n",
    "    # Stratification is possible\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(f\"Split performed with stratification. Minimum samples per class: {min_samples_per_class}\")\n",
    "else:\n",
    "    # Use stratification if possible, otherwise split without it\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    except ValueError:\n",
    "        # If stratification is not possible due to insufficient samples in a class, split without stratification\n",
    "        print(\"Stratified split not possible due to insufficient samples in a class. Splitting without stratification.\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logger.info(\"8. Training Modeling: Starting model training...\")\n",
    "models = train_tree_models(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Evaluation Modeling: evaluasi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X_test, y_test):\n",
    "    \"\"\"Evaluate trained models using appropriate metrics for classification\"\"\"\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate multiple metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"{name} evaluation completed\")\n",
    "        print(f\"\\n{name} Performance Metrics:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "        print(f\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return results\n",
    "\n",
    "logger.info(\"9. Evaluation Modeling: Evaluating models...\")\n",
    "results = evaluate_models(models, X_test_scaled, y_test)\n",
    "\n",
    "# Find the best model based on F1 score\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['f1_score'])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "logger.info(f\"Best model: {best_model_name} with F1-Score: {results[best_model_name]['f1_score']:.4f}\")\n",
    "\n",
    "# Perform feature importance analysis for the best model\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    logger.info(\"Performing feature importance analysis...\")\n",
    "    feature_names = X.columns\n",
    "    importances = best_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    logger.info(\"Top 10 most important features:\")\n",
    "    for in range(min(10, len(feature_names))):\n",
    "        idx = indices[i]\n",
    "        logger.info(f\"{i+1}. {feature_names[idx]}: {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Save Model: menyimpan model terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, output_path):\n",
    "    \"\"\"Save model results to output directory\"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        # Save classification report\n",
    "        report_df = pd.DataFrame(result['classification_report']).transpose()\n",
    "        report_df.to_csv(os.path.join(output_path, f\"{model_name}_report.csv\"))\n",
    "        \n",
    "        # Save confusion matrix\n",
    "        cm_df = pd.DataFrame(result['confusion_matrix'])\n",
    "        cm_df.to_csv(os.path.join(output_path, f\"{model_name}_confusion_matrix.csv\"))\n",
    "        \n",
    "        # Save summary metrics\n",
    "        metrics_df = pd.DataFrame({\n",
    "            'Model': [model_name],\n",
    "            'Accuracy': [result['accuracy']],\n",
    "            'Precision': [result['precision']],\n",
    "            'Recall': [result['recall']],\n",
    "            'F1_Score': [result['f1_score']]\n",
    "        })\n",
    "        metrics_df.to_csv(os.path.join(output_path, f\"{model_name}_metrics.csv\"), index=False)\n",
    "    \n",
    "    logger.info(f\"Results saved to {output_path}\")\n",
    "\n",
    "def save_model(model, model_name, label_encoders, target_encoder, scaler=None):\n",
    "    \"\"\"Save the trained model and preprocessors\"\"\"\n",
    "    output_path = Config().OUTPUT_PATH\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = os.path.join(output_path, f\"{model_name.lower().replace(' ', '_')}_model.pkl\")\n",
    "    joblib.dump(model, model_path)\n",
    "    logger.info(f\"Saved model to {model_path}\")\n",
    "    \n",
    "    # Save label encoders\n",
    "    encoders_path = os.path.join(output_path, \"label_encoders.pkl\")\n",
    "    joblib.dump(label_encoders, encoders_path)\n",
    "    logger.info(f\"Saved label encoders to {encoders_path}\")\n",
    "    \n",
    "    # Save target encoder if it exists\n",
    "    if target_encoder is not None:\n",
    "        target_encoder_path = os.path.join(output_path, \"target_encoder.pkl\")\n",
    "        joblib.dump(target_encoder, target_encoder_path)\n",
    "        logger.info(f\"Saved target encoder to {target_encoder_path}\")\n",
    "    \n",
    "    # Save scaler if provided\n",
    "    if scaler is not None:\n",
    "        scaler_path = os.path.join(output_path, \"scaler.pkl\")\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        logger.info(f\"Saved scaler to {scaler_path}\")\n",
    "\n",
    "logger.info(\"10. Save Model: Saving the best model and results...\")\n",
    "save_model(best_model, best_model_name, label_encoders, target_encoder, scaler)\n",
    "\n",
    "# Save all results\n",
    "config = Config()\n",
    "save_results(results, config.OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Insight & Conclusion: Penarikan kesimpulan dan rekomendasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"11. Insight & Conclusion: Drawing conclusions and recommendations...\")\n",
    "logger.info(\"Analysis completed with the following key points:\")\n",
    "logger.info(\"- EDA has been performed before preprocessing to understand data characteristics\")\n",
    "logger.info(\"- Multiple tree-based models have been trained and evaluated\")\n",
    "logger.info(\"- The best performing model has been saved for future use\")\n",
    "logger.info(\"- Feature importance analysis provides insights into key factors affecting predictions\")\n",
    "\n",
    "logger.info(\"BPJS antrol analysis completed successfully!\")\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "print(f\"F1-Score: {results[best_model_name]['f1_score']:.4f}\")\n",
    "print(f\"Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"Precision: {results[best_model_name]['precision']:.4f}\")\n",
    "print(f\"Recall: {results[best_model_name]['recall']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
