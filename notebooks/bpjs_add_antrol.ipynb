{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPJS Add Antrol Analysis - Jupyter Notebook\n",
    "\n",
    "## 1. Understanding Business\n",
    "\n",
    "Memahami kebutuhan bisnis dan tujuan analisis:\n",
    "\n",
    "Tujuan dari analisis ini adalah untuk melakukan analisis komperenshif identifikasi pendaftaran pasien BPJS Add Antroll. Ini merupakan proyek klasifikasi (Supervised Learning) yang bertujuan untuk menganalisis pola dan faktor-faktor yang mempengaruhi pendaftaran pasien BPJS di sistem antrean rumah sakit. Dengan analisis ini, kita berharap dapat:\n",
    "\n",
    "1. Mengidentifikasi faktor-faktor yang mempengaruhi pendaftaran antrean BPJS\n",
    "2. Memprediksi kecenderungan pasien untuk mendaftar antrean\n",
    "3. Membantu manajemen rumah sakit dalam perencanaan kapasitas layanan\n",
    "4. Memberikan wawasan untuk meningkatkan efisiensi sistem antrean BPJS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n",
    "\n",
    "Menjelajahi dan memahami struktur data:\n",
    "\n",
    "Dataset ini berisi informasi pendaftaran pasien BPJS di sistem antrean rumah sakit. Data ini diperoleh dari database rumah sakit dengan struktur query sebagai berikut:\n",
    "\n",
    "- Informasi registrasi pasien (no_rawat, tgl_registrasi, jam_reg)\n",
    "- Informasi dokter (kd_dokter, nm_dokter)\n",
    "- Informasi pasien (no_rkm_medis, nm_pasien)\n",
    "- Informasi poliklinik (kd_poli, nm_poli)\n",
    "- Informasi penjamin (kd_pj, png_jawab)\n",
    "- Informasi antrean (tanggal_periksa, nomor_kartu, nomor_referensi, kodebooking)\n",
    "- Informasi kunjungan (jenis_kunjungan, status_kirim, keterangan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'database'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m sys.path.append(os.path.abspath(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatabase\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatabase_connection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatabaseConnection, get_bpjs_antrol_data\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Define date range for data extraction\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'database'"
     ]
    }
   ],
   "source": [
    "# Load data from the database connection or CSV fallback\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "from database.database_connection import DatabaseConnection, get_bpjs_antrol_data\n",
    "from config.config import Config\n",
    "\n",
    "# Define date range for data extraction\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Try to load data from database\n",
    "try:\n",
    "    print(f\"Loading data from database for period {start_date} to {end_date}\")\n",
    "    df = get_bpjs_antrol_data(start_date, end_date)\n",
    "    print(f\"Successfully loaded {len(df)} records from database\")\n",
    "except Exception as e:\n",
    "    print(f\"Database connection failed: {e}\")\n",
    "    print(\"Loading data from CSV fallback: database/bpjs antrol.csv\")\n",
    "    # Fallback to CSV file\n",
    "    try:\n",
    "        df = pd.read_csv('database/bpjs antrol.csv')\n",
    "        print(f\"Successfully loaded {len(df)} records from CSV\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"CSV file not found, creating sample data for demonstration\")\n",
    "        # Create sample data for demonstration purposes\n",
    "        sample_data = {\n",
    "            'no_rawat': [f'20230101{str(i).zfill(6)}' for i in range(100)],\n",
    "            'tgl_registrasi': pd.date_range('2023-01-01', periods=100, freq='D').strftime('%Y-%m-%d'),\n",
    "            'jam_reg': [f'{str(i%24).zfill(2)}:{str(i%60).zfill(2)}:00' for i in range(100)],\n",
    "            'kd_dokter': [f'DR{i%10:03d}' for i in range(100)],\n",
    "            'nm_dokter': [f'Dr. {name}' for name in ['Ahmad', 'Budi', 'Citra', 'Dedi', 'Eka', 'Fani', 'Gani', 'Hani', 'Iwan', 'Joko'] * 10],\n",
    "            'no_rkm_medis': [f'RM{i:06d}' for i in range(100)],\n",
    "            'nm_pasien': [f'Pasien {i}' for i in range(100)],\n",
    "            'kd_poli': [f'Poli{i%5:03d}' for i in range(100)],\n",
    "            'nm_poli': ['Poli Umum', 'Poli Gigi', 'Poli Mata', 'Poli Jantung', 'Poli Anak'] * 20,\n",
    "            'status_lanjut': ['Ralan'] * 100,\n",
    "            'kd_pj': ['BPJS'] * 80 + ['UMUM'] * 20,\n",
    "            'png_jawab': ['BPJS'] * 80 + ['UMUM'] * 20,\n",
    "            'tanggal_periksa': pd.date_range('2023-01-01', periods=100, freq='D').strftime('%Y-%m-%d'),\n",
    "            'nomor_kartu': [f'BPJS{i:013d}' for i in range(100)],\n",
    "            'nomor_referensi': [f'REF{i:08d}' for i in range(100)],\n",
    "            'kodebooking': [f'BOOK{i:08d}' for i in range(100)],\n",
    "            'jenis_kunjungan': ['1'] * 70 + ['2'] * 30,\n",
    "            'status_kirim': ['Sudah'] * 90 + ['Belum'] * 10,\n",
    "            'keterangan': [''] * 100,\n",
    "            'USER': ['admin'] * 100\n",
    "        }\n",
    "        df = pd.DataFrame(sample_data)\n",
    "        print(f\"Created sample data with {len(df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation / Wrangling\n",
    "\n",
    "Menyiapkan dan mengolah data:\n",
    "\n",
    "Pada tahap ini kita akan:\n",
    "- Memeriksa struktur data\n",
    "- Mengidentifikasi kolom-kolom yang relevan\n",
    "- Mengubah tipe data jika diperlukan\n",
    "- Membuat fitur baru jika diperlukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime\n",
    "date_columns = ['tgl_registrasi', 'tanggal_periksa']\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "# Create additional features\n",
    "if 'tgl_registrasi' in df.columns:\n",
    "    df['hari_registrasi'] = df['tgl_registrasi'].dt.day_name()\n",
    "    df['bulan_registrasi'] = df['tgl_registrasi'].dt.month\n",
    "    df['tahun_registrasi'] = df['tgl_registrasi'].dt.year\n",
    "\n",
    "if 'tanggal_periksa' in df.columns:\n",
    "    df['hari_periksa'] = df['tanggal_periksa'].dt.day_name()\n",
    "    df['bulan_periksa'] = df['tanggal_periksa'].dt.month\n",
    "    df['tahun_periksa'] = df['tanggal_periksa'].dt.year\n",
    "\n",
    "# Calculate difference between registration and examination dates\n",
    "if 'tgl_registrasi' in df.columns and 'tanggal_periksa' in df.columns:\n",
    "    df['hari_antara_reg_periksa'] = (df['tanggal_periksa'] - df['tgl_registrasi']).dt.days\n",
    "\n",
    "print(\"Data preparation completed\")\n",
    "print(\"New features created:\")\n",
    "new_features = ['hari_registrasi', 'bulan_registrasi', 'tahun_registrasi', \n",
    "                'hari_periksa', 'bulan_periksa', 'tahun_periksa', 'hari_antara_reg_periksa']\n",
    "for feature in new_features:\n",
    "    if feature in df.columns:\n",
    "        print(f\"- {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning\n",
    "\n",
    "Membersihkan data dari ketidakkonsistenan:\n",
    "\n",
    "Pada tahap ini kita akan:\n",
    "- Menangani nilai-nilai yang hilang\n",
    "- Mengidentifikasi dan menangani outlier\n",
    "- Membersihkan data yang tidak konsisten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "\n",
    "# For numerical columns, fill with median\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "        print(f\"Filled missing values in {col} with median\")\n",
    "\n",
    "# For categorical columns, fill with mode\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown', inplace=True)\n",
    "        print(f\"Filled missing values in {col} with mode or 'Unknown'\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicates}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Removed {duplicates} duplicate rows\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(f\"\\nMissing values after cleaning:\")\n",
    "print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explanatory Data Analysis (EDA Deskriptif)\n",
    "\n",
    "Analisis deskriptif awal:\n",
    "\n",
    "Melakukan analisis deskriptif awal untuk memahami distribusi data dan statistik dasar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"Descriptive Statistics for Numerical Variables:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nDescriptive Statistics for Categorical Variables:\")\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols[:5]:  # Show first 5 categorical columns\n",
    "    print(f\"\\n{col} value counts:\")\n",
    "    print(df[col].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribution of registration dates\n",
    "if 'tgl_registrasi' in df.columns:\n",
    "    df['tgl_registrasi'].dt.date.value_counts().sort_index().plot(kind='line', ax=axes[0,0], title='Registrations Over Time')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Distribution of polyclinics\n",
    "if 'nm_poli' in df.columns:\n",
    "    df['nm_poli'].value_counts().head(10).plot(kind='bar', ax=axes[0,1], title='Top 10 Polyclinics by Registration Count')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Distribution of payment methods\n",
    "if 'png_jawab' in df.columns:\n",
    "    df['png_jawab'].value_counts().plot(kind='bar', ax=axes[1,0], title='Registrations by Payment Method')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Distribution of visit types\n",
    "if 'jenis_kunjungan' in df.columns:\n",
    "    df['jenis_kunjungan'].value_counts().plot(kind='bar', ax=axes[1,1], title='Visit Types Distribution')\n",
    "    axes[1,1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (EDA Mendalam)\n",
    "\n",
    "Eksplorasi data secara mendalam:\n",
    "\n",
    "Melakukan eksplorasi data secara lebih mendalam untuk mengidentifikasi pola, hubungan antar variabel, dan wawasan penting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for numerical variables\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "if not numeric_df.empty:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "    plt.title('Correlation Matrix of Numerical Variables')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numerical variables found for correlation analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationships between categorical variables\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Polyclinic vs Payment Method\n",
    "if 'nm_poli' in df.columns and 'png_jawab' in df.columns:\n",
    "    cross_tab = pd.crosstab(df['nm_poli'], df['png_jawab'])\n",
    "    cross_tab_pct = cross_tab.div(cross_tab.sum(1).astype(float), axis=0) * 100\n",
    "    cross_tab_pct.plot(kind='bar', stacked=True, ax=axes[0,0], title='Payment Method by Polyclinic (%)')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].legend(title='Payment Method', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Visit type by day of week\n",
    "if 'jenis_kunjungan' in df.columns and 'hari_registrasi' in df.columns:\n",
    "    cross_tab2 = pd.crosstab(df['hari_registrasi'], df['jenis_kunjungan'])\n",
    "    cross_tab2.plot(kind='bar', ax=axes[0,1], title='Visit Types by Day of Week')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Registration by day of week\n",
    "if 'hari_registrasi' in df.columns:\n",
    "    df['hari_registrasi'].value_counts().plot(kind='bar', ax=axes[1,0], title='Registrations by Day of Week')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Registration by month\n",
    "if 'bulan_registrasi' in df.columns:\n",
    "    df['bulan_registrasi'].value_counts().sort_index().plot(kind='line', marker='o', ax=axes[1,1], title='Registrations by Month')\n",
    "    axes[1,1].set_xlabel('Month')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Preprocessing\n",
    "\n",
    "Pra-pemrosesan data untuk modeling:\n",
    "\n",
    "Melakukan pra-pemrosesan data untuk mempersiapkan data dalam format yang sesuai untuk modeling machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select features for modeling\n",
    "feature_columns = []\n",
    "\n",
    "# Add numerical features\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove date-related columns from features (keep only derived features)\n",
    "date_cols = ['tahun_registrasi', 'tahun_periksa']\n",
    "numeric_features = [col for col in numeric_features if col not in date_cols]\n",
    "feature_columns.extend(numeric_features)\n",
    "\n",
    "# Add categorical features that are relevant\n",
    "categorical_features = ['nm_poli', 'png_jawab', 'hari_registrasi', 'status_lanjut', 'jenis_kunjungan']\n",
    "for col in categorical_features:\n",
    "    if col in df.columns:\n",
    "        feature_columns.append(col)\n",
    "\n",
    "print(f\"Selected features for modeling: {feature_columns}\")\n",
    "\n",
    "# Prepare the feature matrix X\n",
    "X = df[feature_columns].copy()\n",
    "\n",
    "# Handle categorical variables with label encoding\n",
    "label_encoders = {}\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"Encoded {col} with {len(le.classes_)} unique values\")\n",
    "\n",
    "# Create a target variable based on the data\n",
    "# For demonstration, let's create a binary target based on payment method (BPJS vs others)\n",
    "if 'png_jawab' in df.columns:\n",
    "    target_encoder = LabelEncoder()\n",
    "    y = target_encoder.fit_transform(df['png_jawab'].astype(str))\n",
    "    print(f\"Created target variable from 'png_jawab' with classes: {target_encoder.classes_}\")\n",
    "else:\n",
    "    # Create a default target if 'png_jawab' is not available\n",
    "    y = np.random.randint(0, 2, size=len(df))\n",
    "    print(\"Created random binary target variable\")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(f\"Final feature matrix shape: {X_scaled.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Modeling\n",
    "\n",
    "melatih model:\n",
    "\n",
    "Melatih minimal 2 (dua) model machine learning yang relevan dengan jalur proyek Anda. Klasifikasi: Tree-Based Algorithm menggunakan model Machine Learning (Decision Tree, Random Forest dan Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100, max_depth=5)\n",
    "}\n",
    "\n",
    "# Train models\n",
    "trained_models = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    print(f\"{name} training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation Modeling\n",
    "\n",
    "evaluasi model:\n",
    "\n",
    "Mengevaluasi performa model Anda menggunakan metrik evaluasi yang sesuai: Klasifikasi: Accuracy, Precision, Recall, F1-Score, Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "model_performance = {}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Store performance\n",
    "    model_performance[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'predictions': y_pred,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name} Performance Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "performance_df = pd.DataFrame({\n",
    "    name: [metrics['accuracy'], metrics['precision'], metrics['recall'], metrics['f1_score']] \n",
    "    for name, metrics in model_performance.items()\n",
    "}).T\n",
    "performance_df.columns = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "performance_df = performance_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(performance_df)\n",
    "\n",
    "# Visualize model comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "metrics_df = performance_df.T\n",
    "sns.heatmap(metrics_df, annot=True, fmt='.4f', cmap='viridis')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Metrics')\n",
    "plt.xlabel('Models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model\n",
    "\n",
    "menyimpan model terbaik:\n",
    "\n",
    "Simpan (ekspor) model terbaik Anda (bersama dengan preprocessor seperti scaler/encoder) ke dalam sebuah file. (Contoh: model_terbaik.pkl menggunakan pickle atau joblib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Find the best model based on F1 score\n",
    "best_model_name = max(model_performance.keys(), key=lambda k: model_performance[k]['f1_score'])\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"Best model: {best_model_name} with F1-Score: {model_performance[best_model_name]['f1_score']:.4f}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "model_path = os.path.join('output', f'best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl')\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Saved best model to: {model_path}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_path = os.path.join('output', 'scaler.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Saved scaler to: {scaler_path}\")\n",
    "\n",
    "# Save the label encoders\n",
    "encoders_path = os.path.join('output', 'label_encoders.pkl')\n",
    "joblib.dump(label_encoders, encoders_path)\n",
    "print(f\"Saved label encoders to: {encoders_path}\")\n",
    "\n",
    "# Save the target encoder if it exists\n",
    "if 'target_encoder' in locals():\n",
    "    target_encoder_path = os.path.join('output', 'target_encoder.pkl')\n",
    "    joblib.dump(target_encoder, target_encoder_path)\n",
    "    print(f\"Saved target encoder to: {target_encoder_path}\")\n",
    "\n",
    "print(\"All models and preprocessors saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Insight & Conclusion\n",
    "\n",
    "Penarikan kesimpulan dan rekomendasi:\n",
    "\n",
    "Berdasarkan analisis dan modeling yang telah dilakukan, berikut adalah wawasan dan kesimpulan utama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for the best model\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"Top 10 Most Important Features for {best_model_name}:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance.head(10), x='importance', y='feature')\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"### Kesimpulan dan Rekomendasi:\")\n",
    "print(\"\")\n",
    "print(\"1. **Business Understanding**: Proyek ini berhasil menganalisis pola pendaftaran pasien BPJS di sistem antrean rumah sakit.\")\n",
    "print(\"Tujuan utamanya adalah untuk memahami faktor-faktor yang mempengaruhi pendaftaran antrean BPJS dan memprediksi jenis pembayaran pasien.\")\n",
    "print(\"\")\n",
    "print(f\"2. **Model Performance**: Dari ketiga model yang diuji (Decision Tree, Random Forest, dan Gradient Boosting),\")\n",
    "print(f\"model terbaik adalah {best_model_name} dengan F1-Score: {model_performance[best_model_name]['f1_score']:.4f}.\")\n",
    "print(\"\")\n",
    "print(\"3. **Feature Importance**: Analisis menunjukkan bahwa fitur-fitur tertentu memiliki pengaruh besar terhadap prediksi,\")\n",
    "if 'feature_importance' in locals():\n",
    "    top_features = feature_importance.head(3)['feature'].tolist()\n",
    "    print(f\"seperti {', '.join(top_features)}.\")\n",
    "else:\n",
    "    print(\"seperti jenis kunjungan, poliklinik, dan hari registrasi.\")\n",
    "print(\"\")\n",
    "print(\"4. **Rekomendasi Bisnis:\")\n",
    "print(\"   - Fokus pada poliklinik dengan tingkat pendaftaran BPJS tertinggi untuk optimalisasi layanan\")\n",
    "print(\"   - Perhatikan pola pendaftaran harian untuk perencanaan sumber daya\")\n",
    "print(\"   - Gunakan model untuk memprediksi beban pendaftaran di masa depan\")\n",
    "print(\"   - Evaluasi kembali kebijakan penjaminan untuk meningkatkan efisiensi\")\n",
    "print(\"\")\n",
    "print(\"5. **Rekomendasi Teknis:\")\n",
    "print(\"   - Teruskan pengumpulan data untuk meningkatkan akurasi model\")\n",
    "print(\"   - Terapkan teknik feature engineering lebih lanjut\")\n",
    "print(\"   - Evaluasi model secara berkala untuk memastikan kinerja tetap optimal\")\n",
    "print(\"   - Pertimbangkan teknik ensemble lebih canggih untuk peningkatan akurasi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
